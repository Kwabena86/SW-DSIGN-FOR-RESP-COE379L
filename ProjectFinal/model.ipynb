{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    " \n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    # for x in unique_list:\n",
    "    #     print x,\n",
    "    return(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c335ece97634be5873c5bb1527c2542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 171k/171k [00:00<00:00, 1.97MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 42.5k/42.5k [00:00<00:00, 714kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70afe2298c024110977b6022f88977fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee71ad22954a4ff3a3237710e98e689e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gretelai/symptom_to_diagnosis\")\n",
    "\n",
    "train = dataset[\"train\"]\n",
    "train_label = train[:][\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'typhoid': 0, 'fungal infection': 1, 'jaundice': 2, 'allergy': 3, 'varicose veins': 4, 'diabetes': 5, 'malaria': 6, 'hypertension': 7, 'migraine': 8, 'urinary tract infection': 9, 'arthritis': 10, 'gastroesophageal reflux disease': 11, 'impetigo': 12, 'chicken pox': 13, 'peptic ulcer disease': 14, 'pneumonia': 15, 'psoriasis': 16, 'cervical spondylosis': 17, 'dengue': 18, 'bronchial asthma': 19, 'drug reaction': 20, 'common cold': 21}\n"
     ]
    }
   ],
   "source": [
    "labels=(unique(train_label))\n",
    "labels_dict = {labels[i]: i for i in range(len(labels))}\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f234a2e2821442489b1ad3c2bab3f9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ff14a4e9b04786aa7326fee4fae6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 853\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 212\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 # can experiment with different sizes\n",
    "dataset = dataset.rename_column(\"output_text\", \"labels\")\n",
    "dataset = dataset.class_encode_column(\"labels\")\n",
    "dataset = dataset.rename_column(\"input_text\", \"text\")\n",
    "# dataset = dataset.map(input_columns=\"output_text\", remove_columns=\"output_text\", batched=True, batch_size=batch_size)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ClassLabel(names=['allergy', 'arthritis', 'bronchial asthma', 'cervical spondylosis', 'chicken pox', 'common cold', 'dengue', 'diabetes', 'drug reaction', 'fungal infection', 'gastroesophageal reflux disease', 'hypertension', 'impetigo', 'jaundice', 'malaria', 'migraine', 'peptic ulcer disease', 'pneumonia', 'psoriasis', 'typhoid', 'urinary tract infection', 'varicose veins'], id=None), 'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 853\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'text'],\n",
      "        num_rows: 212\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train = dataset[\"train\"]\n",
    "# train_label = train[:][\"label\"]\n",
    "# print(train_label)\n",
    "\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "# test_valid = dataset[\"test\"].train_test_split(0.5)\n",
    "\n",
    "# train_test_valid_dataset = DatasetDict({\n",
    "#     'train': train,\n",
    "#     'test': test_valid['test'],\n",
    "#     'validation': test_valid['train']})\n",
    "\n",
    "train_test_valid_dataset = dataset\n",
    "\n",
    "print(train_test_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66dbdbca7e74a869bf1732ad5a4b886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d8ab6476d74680b8a8ae03f23539c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a626c9a116a94adab5f8d36ef514cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5aed8cd3224e1e861aa5df4cb49b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\n",
    "\n",
    "def preprocess(sample):\n",
    "    return tokenizer(sample[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df42a11b1b2448b8b378edb7e1cd920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/853 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6f986b596647c4b0016fc73f243459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 853\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 212\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_test_valid_dataset.map(preprocess, batched=True, batch_size=batch_size)\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 23:27:21.316629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 23:27:25.846486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caa27068313434eafb1682200d3fb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "metric = load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# batch_size = 110 # can experiment with different sizes\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test\", # directory to save the model\n",
    "    evaluation_strategy = \"epoch\", # evaluate after each epoch\n",
    "    save_strategy = \"epoch\", # save after each epoch\n",
    "    learning_rate=2e-5, # the learning rate to use\n",
    "    per_device_train_batch_size=batch_size, # the batch size\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20, # number of epochs; 5 took about 30 minutes\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\" # metric associated with COLA GLUE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'psoriasis', 1: 'urinary tract infection', 2: 'fungal infection', 3: 'allergy', 4: 'peptic ulcer disease', 5: 'gastroesophageal reflux disease', 6: 'varicose veins', 7: 'hypertension', 8: 'impetigo', 9: 'chicken pox', 10: 'common cold', 11: 'cervical spondylosis', 12: 'bronchial asthma', 13: 'typhoid', 14: 'drug reaction', 15: 'jaundice', 16: 'dengue', 17: 'diabetes', 18: 'pneumonia', 19: 'arthritis', 20: 'malaria', 21: 'migraine'}\n",
      "{'psoriasis': 0, 'urinary tract infection': 1, 'fungal infection': 2, 'allergy': 3, 'peptic ulcer disease': 4, 'gastroesophageal reflux disease': 5, 'varicose veins': 6, 'hypertension': 7, 'impetigo': 8, 'chicken pox': 9, 'common cold': 10, 'cervical spondylosis': 11, 'bronchial asthma': 12, 'typhoid': 13, 'drug reaction': 14, 'jaundice': 15, 'dengue': 16, 'diabetes': 17, 'pneumonia': 18, 'arthritis': 19, 'malaria': 20, 'migraine': 21}\n"
     ]
    }
   ],
   "source": [
    "# autoload a model from the base for sequence classification,\n",
    "# we pass 22 labels\n",
    "label2id = labels_dict\n",
    "id2label = {v: k for k, v in labels_dict.items()}\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642aaf335b64481cb4f094da15b6da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels_dict))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,  # the pre-trained model\n",
    "    args,  # the TrainingAgruments, defined above\n",
    "    train_dataset=tokenized_dataset[\"train\"], # the training dataset\n",
    "    eval_dataset=tokenized_dataset[\"test\"], # the validation dataset\n",
    "    tokenizer=tokenizer, # our tokenizer\n",
    "    data_collator=data_collator, # the collator we defined above\n",
    "    compute_metrics=compute_metrics # our function for computing the metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 13:54, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.763383</td>\n",
       "      <td>0.405660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.124238</td>\n",
       "      <td>0.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.542863</td>\n",
       "      <td>0.797170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.133653</td>\n",
       "      <td>0.853774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818858</td>\n",
       "      <td>0.896226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580683</td>\n",
       "      <td>0.929245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.428355</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.320332</td>\n",
       "      <td>0.952830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249188</td>\n",
       "      <td>0.933962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.192927</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.170397</td>\n",
       "      <td>0.957547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.159446</td>\n",
       "      <td>0.952830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.157728</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.142808</td>\n",
       "      <td>0.952830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.142589</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.136124</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.137319</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>0.136228</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.136175</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.134692</td>\n",
       "      <td>0.948113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.5842200645694027, metrics={'train_runtime': 835.3072, 'train_samples_per_second': 20.424, 'train_steps_per_second': 1.293, 'total_flos': 260102697721620.0, 'train_loss': 0.5842200645694027, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 10% test + valid in half test, half valid\n",
    "dataset2_train = tokenized_dataset[\"train\"].train_test_split(0.5)\n",
    "dataset2_test = tokenized_dataset[\"test\"]\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset2 = DatasetDict({\n",
    "    'train': dataset2_train[\"train\"],\n",
    "    'test': dataset2_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args2 = TrainingArguments(\n",
    "    \"model_2\", # directory to save the model\n",
    "    evaluation_strategy = \"epoch\", # evaluate after each epoch\n",
    "    save_strategy = \"no\", # save after each epoch\n",
    "    learning_rate=2e-5, # the learning rate to use\n",
    "    per_device_train_batch_size=batch_size, # the batch size\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20, # number of epochs; 5 took about 30 minutes\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\" # metric associated with COLA GLUE\n",
    ")\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels_dict))\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model2,  # the pre-trained model\n",
    "    args2,  # the TrainingAgruments, defined above\n",
    "    train_dataset=dataset2[\"train\"], # the training dataset\n",
    "    eval_dataset=dataset2[\"test\"], # the validation dataset\n",
    "    tokenizer=tokenizer, # our tokenizer\n",
    "    data_collator=data_collator, # the collator we defined above\n",
    "    compute_metrics=compute_metrics # our function for computing the metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 09:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.027924</td>\n",
       "      <td>0.165094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.781996</td>\n",
       "      <td>0.382075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.453523</td>\n",
       "      <td>0.514151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.158650</td>\n",
       "      <td>0.603774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.883384</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.640185</td>\n",
       "      <td>0.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.445317</td>\n",
       "      <td>0.806604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.267467</td>\n",
       "      <td>0.830189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.125404</td>\n",
       "      <td>0.839623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.002831</td>\n",
       "      <td>0.867925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913058</td>\n",
       "      <td>0.877358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.825877</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754501</td>\n",
       "      <td>0.896226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696943</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653872</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.618139</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593692</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.577672</td>\n",
       "      <td>0.900943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.249900</td>\n",
       "      <td>0.566473</td>\n",
       "      <td>0.910377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.249900</td>\n",
       "      <td>0.562634</td>\n",
       "      <td>0.910377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory model_2/checkpoint-27 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-54 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-81 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-108 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-135 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-162 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-189 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-216 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-243 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-270 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-297 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-324 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-351 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-378 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-405 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-432 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-459 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-486 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-513 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory model_2/checkpoint-540 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=540, training_loss=1.1813213630958839, metrics={'train_runtime': 597.8993, 'train_samples_per_second': 14.25, 'train_steps_per_second': 0.903, 'total_flos': 129199191688920.0, 'train_loss': 1.1813213630958839, 'epoch': 20.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Split the 10% test + valid in half test, half valid\n",
    "dataset3_train = dataset2_train[\"train\"].train_test_split(0.5)\n",
    "dataset3_test = tokenized_dataset[\"test\"]\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset3 = DatasetDict({\n",
    "    'train': dataset3_train[\"train\"],\n",
    "    'test': dataset3_test})\n",
    "\n",
    "args3 = TrainingArguments(\n",
    "    \"model_3\", # directory to save the model\n",
    "    evaluation_strategy = \"epoch\", # evaluate after each epoch\n",
    "    save_strategy = \"no\", # save after each epoch\n",
    "    learning_rate=2e-5, # the learning rate to use\n",
    "    per_device_train_batch_size=batch_size, # the batch size\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20, # number of epochs; 5 took about 30 minutes\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\" # metric associated with COLA GLUE\n",
    ")\n",
    "\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels_dict))\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model3,  # the pre-trained model\n",
    "    args3,  # the TrainingAgruments, defined above\n",
    "    train_dataset=dataset3[\"train\"], # the training dataset\n",
    "    eval_dataset=dataset3[\"test\"], # the validation dataset\n",
    "    tokenizer=tokenizer, # our tokenizer\n",
    "    data_collator=data_collator, # the collator we defined above\n",
    "    compute_metrics=compute_metrics # our function for computing the metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 04:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.063155</td>\n",
       "      <td>0.108491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.989156</td>\n",
       "      <td>0.183962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.854903</td>\n",
       "      <td>0.301887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.706358</td>\n",
       "      <td>0.377358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.559025</td>\n",
       "      <td>0.481132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.424546</td>\n",
       "      <td>0.523585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.289904</td>\n",
       "      <td>0.580189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.167504</td>\n",
       "      <td>0.589623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.065449</td>\n",
       "      <td>0.608491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.968374</td>\n",
       "      <td>0.627358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.886623</td>\n",
       "      <td>0.655660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.818376</td>\n",
       "      <td>0.655660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.756465</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.703621</td>\n",
       "      <td>0.683962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.665330</td>\n",
       "      <td>0.679245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.630109</td>\n",
       "      <td>0.688679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.605523</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.587298</td>\n",
       "      <td>0.688679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.575571</td>\n",
       "      <td>0.693396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.570793</td>\n",
       "      <td>0.693396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=280, training_loss=1.8242047991071428, metrics={'train_runtime': 240.9919, 'train_samples_per_second': 17.677, 'train_steps_per_second': 1.162, 'total_flos': 62451150382392.0, 'train_loss': 1.8242047991071428, 'epoch': 20.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Split the 10% test + valid in half test, half valid\n",
    "dataset4_train = dataset3_train[\"train\"].train_test_split(0.5)\n",
    "dataset4_test = tokenized_dataset[\"test\"]\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset4 = DatasetDict({\n",
    "    'train': dataset4_train[\"train\"],\n",
    "    'test': dataset4_test})\n",
    "\n",
    "args4 = TrainingArguments(\n",
    "    f\"model_4\", # directory to save the model\n",
    "    evaluation_strategy = \"epoch\", # evaluate after each epoch\n",
    "    save_strategy = \"no\", # save after each epoch\n",
    "    learning_rate=2e-5, # the learning rate to use\n",
    "    per_device_train_batch_size=batch_size, # the batch size\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20, # number of epochs; 5 took about 40 minutes\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\" # metric associated with COLA GLUE\n",
    ")\n",
    "\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels_dict))\n",
    "\n",
    "trainer4 = Trainer(\n",
    "    model4,  # the pre-trained model\n",
    "    args4,  # the TrainingAgruments, defined above\n",
    "    train_dataset=dataset4[\"train\"], # the training dataset\n",
    "    eval_dataset=dataset4[\"test\"], # the validation dataset\n",
    "    tokenizer=tokenizer, # our tokenizer\n",
    "    data_collator=data_collator, # the collator we defined above\n",
    "    compute_metrics=compute_metrics # our function for computing the metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 02:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.090332</td>\n",
       "      <td>0.070755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.081134</td>\n",
       "      <td>0.061321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.069970</td>\n",
       "      <td>0.070755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.047992</td>\n",
       "      <td>0.136792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.021141</td>\n",
       "      <td>0.155660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.977119</td>\n",
       "      <td>0.198113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.928387</td>\n",
       "      <td>0.226415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.881377</td>\n",
       "      <td>0.240566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.835170</td>\n",
       "      <td>0.273585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.783495</td>\n",
       "      <td>0.283019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.740514</td>\n",
       "      <td>0.283019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.701591</td>\n",
       "      <td>0.297170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.667008</td>\n",
       "      <td>0.320755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.636621</td>\n",
       "      <td>0.334906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.617002</td>\n",
       "      <td>0.330189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.596603</td>\n",
       "      <td>0.339623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.577543</td>\n",
       "      <td>0.344340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.566089</td>\n",
       "      <td>0.353774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.559856</td>\n",
       "      <td>0.358491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.557841</td>\n",
       "      <td>0.358491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=2.482023184640067, metrics={'train_runtime': 148.0159, 'train_samples_per_second': 14.323, 'train_steps_per_second': 0.946, 'total_flos': 31468087346112.0, 'train_loss': 2.482023184640067, 'epoch': 20.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/root/.cache/huggingface/diagnosis_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
